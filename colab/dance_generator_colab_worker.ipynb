{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dance Video Generator - Colab Worker\n",
        "\n",
        "This notebook runs as a remote GPU worker for the Dance Video Generator app.\n",
        "\n",
        "## Setup:\n",
        "1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4/V100/A100)\n",
        "2. Run all cells\n",
        "3. Copy the ngrok URL\n",
        "4. Set `COLAB_WORKER_URL` in your backend .env\n",
        "\n",
        "## GPU Tiers:\n",
        "- **Free**: T4 (16GB VRAM) - Good for solo videos\n",
        "- **Colab Pro**: V100 (16GB VRAM) - Better performance\n",
        "- **Colab Pro+**: A100 (40GB VRAM) - Best for groups\n"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q fastapi uvicorn pyngrok torch torchvision diffusers transformers accelerate\n",
        "!pip install -q opencv-python pillow numpy scipy scikit-image imageio imageio-ffmpeg\n",
        "!pip install -q ultralytics mediapipe minio yt-dlp ffmpeg-python\n",
        "\n",
        "print(\"‚úÖ Dependencies installed\")"
      ],
      "metadata": {
        "id": "install-deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "check-gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone model repositories\n",
        "!git clone https://github.com/AliaksandrSiarohin/first-order-model.git /content/fomm\n",
        "!git clone https://github.com/magic-research/magic-animate.git /content/magic-animate\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/fomm')\n",
        "sys.path.append('/content/magic-animate')\n",
        "\n",
        "print(\"‚úÖ Models cloned\")"
      ],
      "metadata": {
        "id": "clone-models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastAPI server code\n",
        "from fastapi import FastAPI, BackgroundTasks\n",
        "from pydantic import BaseModel\n",
        "from typing import Dict, Optional\n",
        "import uuid\n",
        "import asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "app = FastAPI(title=\"Dance Generator Colab Worker\")\n",
        "\n",
        "# Job storage\n",
        "jobs = {}\n",
        "\n",
        "class GenerateRequest(BaseModel):\n",
        "    project_id: str\n",
        "    config: Dict\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"gpu_available\": torch.cuda.is_available(),\n",
        "        \"timestamp\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "\n",
        "@app.get(\"/gpu_info\")\n",
        "def get_gpu_info():\n",
        "    \"\"\"Get GPU information\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return {\"error\": \"No GPU available\"}\n",
        "    \n",
        "    return {\n",
        "        \"name\": torch.cuda.get_device_name(0),\n",
        "        \"memory_total\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
        "        \"memory_allocated\": torch.cuda.memory_allocated(0) / 1e9,\n",
        "        \"memory_reserved\": torch.cuda.memory_reserved(0) / 1e9\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "async def generate_video(\n",
        "    request: GenerateRequest,\n",
        "    background_tasks: BackgroundTasks\n",
        "):\n",
        "    \"\"\"Start video generation job\"\"\"\n",
        "    job_id = str(uuid.uuid4())\n",
        "    \n",
        "    jobs[job_id] = {\n",
        "        \"status\": \"queued\",\n",
        "        \"progress\": 0,\n",
        "        \"step\": \"Initializing\",\n",
        "        \"logs\": []\n",
        "    }\n",
        "    \n",
        "    # Start background task\n",
        "    background_tasks.add_task(\n",
        "        process_video_generation,\n",
        "        job_id,\n",
        "        request.project_id,\n",
        "        request.config\n",
        "    )\n",
        "    \n",
        "    return {\"job_id\": job_id}\n",
        "\n",
        "@app.get(\"/status/{job_id}\")\n",
        "def get_job_status(job_id: str):\n",
        "    \"\"\"Get job status\"\"\"\n",
        "    if job_id not in jobs:\n",
        "        return {\"error\": \"Job not found\"}\n",
        "    \n",
        "    return jobs[job_id]\n",
        "\n",
        "async def process_video_generation(\n",
        "    job_id: str,\n",
        "    project_id: str,\n",
        "    config: Dict\n",
        "):\n",
        "    \"\"\"Process video generation (runs in background)\"\"\"\n",
        "    import time\n",
        "    \n",
        "    try:\n",
        "        # TODO: Implement actual video generation\n",
        "        # For now, simulate processing\n",
        "        \n",
        "        jobs[job_id][\"status\"] = \"processing\"\n",
        "        \n",
        "        for progress in range(0, 101, 10):\n",
        "            time.sleep(3)\n",
        "            jobs[job_id][\"progress\"] = progress\n",
        "            jobs[job_id][\"step\"] = f\"Processing {progress}%\"\n",
        "            jobs[job_id][\"logs\"].append(f\"Progress: {progress}%\")\n",
        "        \n",
        "        jobs[job_id][\"status\"] = \"completed\"\n",
        "        jobs[job_id][\"final_video_url\"] = f\"minio://dance-videos/{project_id}_colab.mp4\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        jobs[job_id][\"status\"] = \"failed\"\n",
        "        jobs[job_id][\"error\"] = str(e)\n",
        "\n",
        "print(\"‚úÖ FastAPI server created\")"
      ],
      "metadata": {
        "id": "fastapi-server"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start ngrok tunnel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token (get from https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "# IMPORTANT: Replace with your token!\n",
        "ngrok_auth_token = \"YOUR_NGROK_TOKEN_HERE\"  # ‚ö†Ô∏è REPLACE THIS\n",
        "\n",
        "if ngrok_auth_token != \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "    \n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"\\nüåê Colab Worker URL: {public_url}\")\n",
        "    print(f\"\\n‚ö†Ô∏è Copy this URL and set it as COLAB_WORKER_URL in your backend .env\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Please set your ngrok auth token above!\")\n",
        "    print(\"Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")"
      ],
      "metadata": {
        "id": "start-ngrok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start FastAPI server\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import uvicorn\n",
        "\n",
        "print(\"\\nüöÄ Starting Colab Worker Server...\\n\")\n",
        "print(\"Worker is ready to receive jobs!\")\n",
        "print(\"Keep this cell running to process videos.\\n\")\n",
        "\n",
        "# Run server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "start-server"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
